
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Exercise 6: Nonlinear Logistic Regression</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-05-06"><meta name="DC.source" content="Exercise6.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Exercise 6: Nonlinear Logistic Regression</h1><!--introduction--><p>Submitted by <b>Prasannjeet Singh</b></p><p>
  <link rel="stylesheet" type="text/css" href="../Data/layout.css">
</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Q1. Plotting</a></li><li><a href="#2">Q2. Non Linear Boundaries</a></li><li><a href="#3">Q3. Gradient Descent</a></li><li><a href="#5">Q4. Using Fminunc</a></li><li><a href="#7">Q5. Graph Plot for multiple degrees</a></li></ul></div><h2 id="1">Q1. Plotting</h2><pre class="codeinput"><span class="comment">% Declaring the degree</span>
d = 2;
load <span class="string">Data/microchiptests.csv</span>;
xMax = max(microchiptests(:,1));
xMin = min(microchiptests(:,1));
yMax = max(microchiptests(:,2));
yMin = min(microchiptests(:,2));
hFig = figure(1);
gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
title(<span class="string">'Quality of Microchipsets'</span>);
xlabel(<span class="string">'Measurement 1'</span>);
ylabel(<span class="string">'Measurement 2'</span>);
legend(<span class="string">'Flawed (0)'</span>, <span class="string">'OK (1)'</span>);
snapnow;
close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise6_01.png" alt=""> <h2 id="2">Q2. Non Linear Boundaries</h2><pre class="codeinput">y = microchiptests(:,3);
X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), d);
</pre><h2 id="3">Q3. Gradient Descent</h2><pre class="codeinput">a = 1;
[b, costArray, N] = ExTwoFunctions.logisticGradient(X,y,a);
N
b
fprintf(strcat(<span class="string">'The final cost is: '</span>, num2str(costArray(end,2))));
</pre><pre class="codeoutput">
N =

        3233


b =

    4.9245
    3.0562
    3.9383
  -11.4517
   -7.0606
  -11.2168

The final cost is:0.34846</pre><p>Summarizing:</p><div><ol><li><img src="Exercise6_eq11007464405666851578.png" alt="$\alpha = 1$"></li><li><img src="Exercise6_eq03936109178036650678.png" alt="$N_{iter} = 3233$"></li><li><img src="Exercise6_eq00733726863918741034.png" alt="$\beta_{0}=4.924500623682294$"></li><li><img src="Exercise6_eq16327792147620399109.png" alt="$\beta_{1}=3.056227942456959$"></li><li><img src="Exercise6_eq11529529194854571739.png" alt="$\beta_{2}=3.938277588229848$"></li><li><img src="Exercise6_eq02660641031245019681.png" alt="$\beta_{3}=11.451677047200493$"></li><li><img src="Exercise6_eq11342025590605373016.png" alt="$\beta_{4}=7.060554469226538$"></li><li><img src="Exercise6_eq08824944730587014364.png" alt="$\beta_{5}=11.216772067542927$"></li></ol></div><p>Now plotting the decision boundary:</p><pre class="codeinput">warning(<span class="string">'off'</span>,<span class="string">'all'</span>);
hFig  = figure(2);
gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
hold <span class="string">on</span>;
f = @(x,y) b' * ExTwoFunctions.mapFeature(x,y,d)';
fimplicit(f,[xMin xMax yMin yMax]);
title(<span class="string">'The Decision Boundary'</span>); legend(<span class="string">'Flawed (0)'</span>, <span class="string">'OK (1)'</span>, <span class="string">'Decision Boundary'</span>);
snapnow;
close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise6_02.png" alt=""> <h2 id="5">Q4. Using Fminunc</h2><pre class="codeinput">options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, 1000,<span class="string">'Display'</span>,<span class="string">'off'</span>);
[n, featCount] = size(X);
b = zeros(featCount,1);
X(:,1) = [];
[beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminunc(beta, X, y)), b, options);

fprintf(strcat(<span class="string">'Iterations:'</span>, 32, int2str(output.iterations),<span class="string">'\r\n'</span>));
fprintf(strcat(<span class="string">'Alpha:'</span>, 32, num2str(output.stepsize),<span class="string">'\r\n'</span>));
fprintf(strcat(<span class="string">'Final Cost:'</span>, 32, num2str(final_cost)));
beta
</pre><pre class="codeoutput">Iterations: 32
Alpha: 0.00031062
Final Cost: 0.34811
beta =

    5.1694
    3.2475
    4.1662
  -12.0268
   -7.5312
  -11.8224

</pre><p>As we can notice, the cost in the implemented function is 0.34846 and the cost in the <b>fminunc()</b> is 0.34811. Therefore, they are almost similar.</p><h2 id="7">Q5. Graph Plot for multiple degrees</h2><pre class="codeinput">hFig = figure(5);
set(hFig, <span class="string">'Position'</span>, [0 0 1500 1500]);
trainingErrorArray(1,:) = [1 1];
aiccNoReg(1,:) = [1 1];
C = log(2*pi)+1;
<span class="keyword">for</span> i = 1:9
<span class="comment">%     i</span>
    X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), i);
    [n, featCount] = size(X);
    b = zeros(featCount,1);
    xPlot = X;
    X(:,1) = [];
    [beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminunc(beta, X, y)), b, options);
    subplot(3,3,i);
    gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
    hold <span class="string">on</span>;
    f = @(x,y) beta' * ExTwoFunctions.mapFeature(x,y,i)';
<span class="comment">%     fimplicit(f,[xMin xMax yMin yMax]);</span>
    fimplicit(f);
<span class="comment">%     i</span>
    curMSE = ExTwoFunctions.costLogistic([ones(n,1) X],y,beta);
    aiccNoReg(i,:) = [i, n*log(curMSE) + 2*featCount + n*C];
    trainingErrorArray(i,:) = [i, sum(((ExTwoFunctions.sigmoid((xPlot) * beta))&gt;= 0.5) ~= y)];
    title(strcat(<span class="string">'Degree:'</span>,32,int2str(i),32,<span class="string">'Error:'</span>,32,num2str(trainingErrorArray(i,2))));
    legend(<span class="string">'Flawed (0)'</span>, <span class="string">'OK (1)'</span>, <span class="string">'Decision Boundary'</span>);
<span class="keyword">end</span>
snapnow;
close(hFig);
save(<span class="string">'tea.mat'</span>, <span class="string">'trainingErrorArray'</span>, <span class="string">'aiccNoReg'</span>);
</pre><img vspace="5" hspace="5" src="Exercise6_03.png" alt=""> <p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Exercise 6: Nonlinear Logistic Regression
% Submitted by *Prasannjeet Singh*
%
% <html>
%   <link rel="stylesheet" type="text/css" href="../Data/layout.css">
% </html>
%
%% Q1. Plotting

% Declaring the degree
d = 2;
load Data/microchiptests.csv;
xMax = max(microchiptests(:,1));
xMin = min(microchiptests(:,1));
yMax = max(microchiptests(:,2));
yMin = min(microchiptests(:,2));
hFig = figure(1);
gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
title('Quality of Microchipsets');
xlabel('Measurement 1');
ylabel('Measurement 2');
legend('Flawed (0)', 'OK (1)');
snapnow;
close(hFig);

%% Q2. Non Linear Boundaries

y = microchiptests(:,3);
X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), d);


%% Q3. Gradient Descent

a = 1;
[b, costArray, N] = ExTwoFunctions.logisticGradient(X,y,a);
N
b
fprintf(strcat('The final cost is: ', num2str(costArray(end,2))));
%%
% Summarizing:
%
% # $\alpha = 1$
% # $N_{iter} = 3233$
% # $\beta_{0}=4.924500623682294$
% # $\beta_{1}=3.056227942456959$
% # $\beta_{2}=3.938277588229848$
% # $\beta_{3}=11.451677047200493$
% # $\beta_{4}=7.060554469226538$
% # $\beta_{5}=11.216772067542927$
%
% Now plotting the decision boundary:

warning('off','all');
hFig  = figure(2);
gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
hold on;
f = @(x,y) b' * ExTwoFunctions.mapFeature(x,y,d)';
fimplicit(f,[xMin xMax yMin yMax]);
title('The Decision Boundary'); legend('Flawed (0)', 'OK (1)', 'Decision Boundary');
snapnow;
close(hFig);


%% Q4. Using Fminunc

options = optimset('GradObj', 'on', 'MaxIter', 1000,'Display','off');
[n, featCount] = size(X);
b = zeros(featCount,1);
X(:,1) = [];
[beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminunc(beta, X, y)), b, options);

fprintf(strcat('Iterations:', 32, int2str(output.iterations),'\r\n'));
fprintf(strcat('Alpha:', 32, num2str(output.stepsize),'\r\n'));
fprintf(strcat('Final Cost:', 32, num2str(final_cost)));
beta

%%
% As we can notice, the cost in the implemented function is 0.34846 and the
% cost in the *fminunc()* is 0.34811. Therefore, they are almost similar.

%% Q5. Graph Plot for multiple degrees

hFig = figure(5);
set(hFig, 'Position', [0 0 1500 1500]);
trainingErrorArray(1,:) = [1 1];
aiccNoReg(1,:) = [1 1];
C = log(2*pi)+1;
for i = 1:9
%     i
    X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), i);
    [n, featCount] = size(X);
    b = zeros(featCount,1);
    xPlot = X;
    X(:,1) = [];
    [beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminunc(beta, X, y)), b, options);
    subplot(3,3,i);
    gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
    hold on;
    f = @(x,y) beta' * ExTwoFunctions.mapFeature(x,y,i)';
%     fimplicit(f,[xMin xMax yMin yMax]);
    fimplicit(f);
%     i
    curMSE = ExTwoFunctions.costLogistic([ones(n,1) X],y,beta);
    aiccNoReg(i,:) = [i, n*log(curMSE) + 2*featCount + n*C];
    trainingErrorArray(i,:) = [i, sum(((ExTwoFunctions.sigmoid((xPlot) * beta))>= 0.5) ~= y)];
    title(strcat('Degree:',32,int2str(i),32,'Error:',32,num2str(trainingErrorArray(i,2)))); 
    legend('Flawed (0)', 'OK (1)', 'Decision Boundary');
end
snapnow;
close(hFig);
save('tea.mat', 'trainingErrorArray', 'aiccNoReg');




##### SOURCE END #####
--></body></html>