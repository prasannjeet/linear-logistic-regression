
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Exercise 8: Forward Selection</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-05-06"><meta name="DC.source" content="Exercise8.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Exercise 8: Forward Selection</h1><!--introduction--><p>Submitted by <b>Prasannjeet Singh</b></p><p>
  <link rel="stylesheet" type="text/css" href="../Data/layout.css">
</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Q1. k-fold cross-validation implementation</a></li><li><a href="#2">Q2. Forward Selection Algorithm</a></li><li><a href="#3">Q3. Applying forward selection</a></li></ul></div><h2 id="1">Q1. k-fold cross-validation implementation</h2><p>The function <b>kFoldCrossValidation()</b> is implemented and is available in this folder</p><h2 id="2">Q2. Forward Selection Algorithm</h2><p>The function <b>forwardSelection()</b> is implemented and is available in this folder</p><h2 id="3">Q3. Applying forward selection</h2><pre class="codeinput">load <span class="string">Data/GPUbenchmark.csv</span>
X = GPUbenchmark;
y = X(:,end);
<span class="comment">% Applying forward selection below</span>
bestModels = ExTwoFunctions.forwardSelection(X);

<span class="comment">% In the cell 'bestModels' we get a list of models for which we will apply</span>
<span class="comment">% k-fold cross validation (k=3). Note that "besetModels" is just a cell containing all the models. So at bestModels{i}, we have the indexes of all the features that are most important, if we choose "i" featurs. Since we are already aware of the model with "0" features, it doesn't return the same.</span>
k = 3;
<span class="keyword">for</span> i = 1:length(bestModels)
    res(i,:) = [i ExTwoFunctions.kFoldCrossValidation([X(:,bestModels{i}) X(:,end)],k)];
<span class="keyword">end</span>

<span class="comment">% Applying k-Fold Cross Validation separately for model with "0" features:</span>
res(length(bestModels)+1,:) = [0 ExTwoFunctions.kFoldCrossValidation([ones(length(y),1) y],k)];
res = sortrows(res,2)
<span class="keyword">if</span> (res(1,1) == 0)
    totalFeatures = 0
    featureList = <span class="string">'Model with 0 Features'</span>
<span class="keyword">else</span>
    totalFeatures = res(1,1)
    featureList = sort(bestModels{totalFeatures})
<span class="keyword">end</span>
</pre><pre class="codeoutput">
res =

   1.0e+03 *

    0.0040    0.1954
    0.0050    0.2257
    0.0060    0.2626
    0.0030    0.2918
    0.0020    0.3552
    0.0010    1.1000
         0    8.0668


totalFeatures =

     4


featureList =

     1     3     5     6

</pre><p>Total number (count) of relevant features is displayed by the variable <b>totalFeatures</b> above. Also the variable <b>featureList</b> lists the features that were selected. The indexes point to their respective columns, or the following features:</p><div><ol><li>Cuda Cores</li><li>Base Clock</li><li>Boost Clock</li><li>Memory Speed</li><li>Memory Config</li><li>Memory Bandwidth</li></ol></div><pre class="codeinput">importantFeature = bestModels{1}
</pre><pre class="codeoutput">
importantFeature =

     5

</pre><p>The most important feature should be the one which was selected in the very first iteration. Which is the feature #5. <b>Memory Config</b>. This can be justified by going back to exercise 2 and looking at the scatter-plots of all the training data. As the regression model used by us is <b>linear</b>, we can clearly observe that the scatter plot of <b>Memory-Config</b> is one plot that is largely concentrated along a straight line. Moreover, it is clearly evident by observation that scatter plots 1, 2 and 3 are not distributed over a straight line. Although we cannot clearly distinguish which plot amongst 3,4 and 5 is the most important, calculation of the mean squared error tells us that feature #5 gives us the least value of cost.</p><p>Note that in case of polynomial regression we might get other answers depending on the degree chosen.</p><p><b>AIC:</b> Following formula was used to calculate the <i>Akaike's Information Criterion</i>:</p><p><img src="Exercise8_eq06774948065882415990.png" alt="$$AIC = n*ln(MSE)+2*d+n*c$$"></p><pre class="codeinput">[n,~] = size(X);
C = log(2*pi)+1;
<span class="keyword">for</span> i = 1:length(bestModels)
    xCur = X(:,bestModels{i});
<span class="comment">%     xCur</span>
    d = size(xCur,2)+1;
    mdl = fitlm(xCur,y);
    mse = ExTwoFunctions.mseMultiFeature(xCur,y,mdl.Coefficients.Estimate);
    aicGPU(i,:) = [i, n*log(mse) + 2*d + n*C];
    aiccGPU(i,:) = [i, aicGPU(i,2)+2*d*(d+1)/(n-d-1)];
<span class="keyword">end</span>

<span class="comment">% Calculating AIC an d AICc separately for models with "0" features</span>
xZero = ones(length(y),1);
mdl = fitlm(xZero,y);
mse = ExTwoFunctions.mseMultiFeature(xZero,y,mdl.Coefficients.Estimate);
aicGPU(length(bestModels)+1,:) = [0, n*log(mse) + 2*d + n*C];
aiccGPU(length(bestModels)+1,:) = [0, aicGPU(i,2)+2*d*(d+1)/(n-d-1)];

<span class="comment">% To see the AIC values of each model, semicolon from the following line</span>
<span class="comment">% can be removed</span>
aicGPU = sortrows(aicGPU,2)
<span class="keyword">if</span> (aicGPU(1,1) == 0)
    bestAicModel = <span class="string">'Zero Features'</span>
<span class="keyword">else</span>
    bestAicModel = sort(bestModels{aicGPU(1,1)})
<span class="keyword">end</span>
</pre><pre class="codeoutput">
aicGPU =

    4.0000  108.8400
    5.0000  108.9409
    6.0000  110.3952
    3.0000  119.6578
    2.0000  125.5525
    1.0000  140.3005
         0  192.1751


bestAicModel =

     1     3     5     6

</pre><p>Therefore, the best model according to AIC contains 4 features (1, 3, 5 and 6), as it has the least AIC value.</p><p><b>AICc:</b> Following formula was used to calculate the <i>AICc</i>:</p><p><img src="Exercise8_eq03187930400897351497.png" alt="$$AICc = AIC + 2d*\frac{d+1}{n-d-1}$$"></p><p><a href="https://se.mathworks.com/help/ident/ref/aic.html#buy66l9-2">Formula Source</a></p><pre class="codeinput"><span class="comment">% To see the AICc values of each model, semicolon from the following line</span>
<span class="comment">% can be removed</span>
aiccGPU = sortrows(aiccGPU,2)
<span class="keyword">if</span> (aiccGPU(1,1) == 0)
    bestAiccModel = <span class="string">'Zero Features'</span>
<span class="keyword">else</span>
    bestAiccModel = sort(bestModels{aiccGPU(1,1)})
<span class="keyword">end</span>
</pre><pre class="codeoutput">
aiccGPU =

    4.0000  113.8400
    5.0000  116.5773
    6.0000  121.5952
         0  121.5952
    3.0000  122.7348
    2.0000  127.2668
    1.0000  141.1005


bestAiccModel =

     1     3     5     6

</pre><p>Although the AICc values are different than AIC values, the best model still remains the same, as the model with 4 features has the least AICc value.</p><p>It is difficult to compare the AIC and AICc results with k-fold cross validation results, as the latter keeps changing each time the function is executed. It is expected, as we shuffle all the rows of the sample data randomly, every time we run the program.</p><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Exercise 8: Forward Selection
% Submitted by *Prasannjeet Singh*
%
% <html>
%   <link rel="stylesheet" type="text/css" href="../Data/layout.css">
% </html>
%
%% Q1. k-fold cross-validation implementation
% The function *kFoldCrossValidation()* is implemented and is available in
% this folder

%% Q2. Forward Selection Algorithm
% The function *forwardSelection()* is implemented and is available in this
% folder

%% Q3. Applying forward selection

load Data/GPUbenchmark.csv
X = GPUbenchmark;
y = X(:,end);
% Applying forward selection below
bestModels = ExTwoFunctions.forwardSelection(X);

% In the cell 'bestModels' we get a list of models for which we will apply
% k-fold cross validation (k=3). Note that "besetModels" is just a cell containing all the models. So at bestModels{i}, we have the indexes of all the features that are most important, if we choose "i" featurs. Since we are already aware of the model with "0" features, it doesn't return the same.
k = 3;
for i = 1:length(bestModels)
    res(i,:) = [i ExTwoFunctions.kFoldCrossValidation([X(:,bestModels{i}) X(:,end)],k)];
end

% Applying k-Fold Cross Validation separately for model with "0" features:
res(length(bestModels)+1,:) = [0 ExTwoFunctions.kFoldCrossValidation([ones(length(y),1) y],k)];
res = sortrows(res,2)
if (res(1,1) == 0)
    totalFeatures = 0
    featureList = 'Model with 0 Features'
else
    totalFeatures = res(1,1)
    featureList = sort(bestModels{totalFeatures})
end

%%
% Total number (count) of relevant features is displayed by the variable
% *totalFeatures* above. Also the variable *featureList* lists the features
% that were selected. The indexes point to their respective columns, or the
% following features:
%
% # Cuda Cores
% # Base Clock
% # Boost Clock
% # Memory Speed
% # Memory Config
% # Memory Bandwidth

importantFeature = bestModels{1}

%%
% The most important feature should be the one which was selected in the
% very first iteration. Which is the feature #5. *Memory Config*. This can
% be justified by going back to exercise 2 and looking at the scatter-plots
% of all the training data. As the regression model used by us is *linear*,
% we can clearly observe that the scatter plot of *Memory-Config* is one
% plot that is largely concentrated along a straight line. Moreover, it is
% clearly evident by observation that scatter plots 1, 2 and 3 are not
% distributed over a straight line. Although we cannot clearly distinguish
% which plot amongst 3,4 and 5 is the most important, calculation of the
% mean squared error tells us that feature #5 gives us the least value of
% cost.
%
% Note that in case of polynomial regression we might get other answers
% depending on the degree chosen.

%%
% *AIC:* Following formula was used to calculate the _Akaike's Information
% Criterion_:
%
% $$AIC = n*ln(MSE)+2*d+n*c$$

[n,~] = size(X);
C = log(2*pi)+1;
for i = 1:length(bestModels)
    xCur = X(:,bestModels{i});
%     xCur
    d = size(xCur,2)+1;
    mdl = fitlm(xCur,y);
    mse = ExTwoFunctions.mseMultiFeature(xCur,y,mdl.Coefficients.Estimate);
    aicGPU(i,:) = [i, n*log(mse) + 2*d + n*C];
    aiccGPU(i,:) = [i, aicGPU(i,2)+2*d*(d+1)/(n-d-1)];
end

% Calculating AIC an d AICc separately for models with "0" features
xZero = ones(length(y),1);
mdl = fitlm(xZero,y);
mse = ExTwoFunctions.mseMultiFeature(xZero,y,mdl.Coefficients.Estimate);
aicGPU(length(bestModels)+1,:) = [0, n*log(mse) + 2*d + n*C];
aiccGPU(length(bestModels)+1,:) = [0, aicGPU(i,2)+2*d*(d+1)/(n-d-1)];

% To see the AIC values of each model, semicolon from the following line
% can be removed
aicGPU = sortrows(aicGPU,2)
if (aicGPU(1,1) == 0)
    bestAicModel = 'Zero Features'
else
    bestAicModel = sort(bestModels{aicGPU(1,1)})
end

%%
% Therefore, the best model according to AIC contains 4 features (1, 3, 5
% and 6), as it has the least AIC value.
%
% *AICc:* Following formula was used to calculate the _AICc_:
%
% $$AICc = AIC + 2d*\frac{d+1}{n-d-1}$$ 
%
% <https://se.mathworks.com/help/ident/ref/aic.html#buy66l9-2 Formula Source>

% To see the AICc values of each model, semicolon from the following line
% can be removed
aiccGPU = sortrows(aiccGPU,2)
if (aiccGPU(1,1) == 0)
    bestAiccModel = 'Zero Features'
else
    bestAiccModel = sort(bestModels{aiccGPU(1,1)})
end


%%
% Although the AICc values are different than AIC values, the best model
% still remains the same, as the model with 4 features has the least AICc
% value.
%
% It is difficult to compare the AIC and AICc results with k-fold cross
% validation results, as the latter keeps changing each time the function
% is executed. It is expected, as we shuffle all the rows of the sample
% data randomly, every time we run the program.
##### SOURCE END #####
--></body></html>