
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Exercise 9: Regularization</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-05-06"><meta name="DC.source" content="Exercise9.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Exercise 9: Regularization</h1><!--introduction--><p>Submitted by <b>Prasannjeet Singh</b></p><p>
  <link rel="stylesheet" type="text/css" href="../Data/layout.css">
</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Q1. Implementing costLogisticReg() and gradientDescentReg()</a></li><li><a href="#3">Q2. Gradient Descent with the Regularization Term</a></li><li><a href="#4">Q3. Making a plot on training error</a></li><li><a href="#6">Q4. Cross Validation</a></li><li><a href="#8">Q5. Trying a larger value of <img src="Exercise9_eq07657233533591063549.png" alt="$\lambda$"></a></li></ul></div><h2 id="1">Q1. Implementing costLogisticReg() and gradientDescentReg()</h2><p>The functions are implemented and are available in this folder.</p><p>Using the cost function (regularization) to calculate cost. The <img src="Exercise9_eq07657233533591063549.png" alt="$\lambda$"> value was considered to be 1.</p><pre class="codeinput">load <span class="string">Data/microchiptests.csv</span>;
X = [ones(size(microchiptests,1),1) microchiptests(:,1:end-1)];
y = microchiptests(:,end);
b = zeros(size(X,2),1);
lambda = 1;
X(:,1) = [];
options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, 1000,<span class="string">'Display'</span>,<span class="string">'off'</span>);
theCost = ExTwoFunctions.costLogisticReg(b,X,y,1)
</pre><pre class="codeoutput">
theCost =

    0.6931

</pre><p>Using the above implemented gradient descent to calculate the beta and cost</p><pre class="codeinput">a = 0.1;
[betaGradientDescent, costArray, ~] = ExTwoFunctions.gradientDescentReg(X,y,a,lambda);
betaGradientDescent
costGradientDescent = costArray(end,2)
</pre><pre class="codeoutput">
betaGradientDescent =

   -0.0217
   -0.2325
   -0.0026


costGradientDescent =

    0.6906

</pre><h2 id="3">Q2. Gradient Descent with the Regularization Term</h2><p>As per the question, the gradient descent has been performed in the same way as done in Exercise-6</p><pre class="codeinput">hFig = figure(2);
set(hFig, <span class="string">'Position'</span>, [0 0 1500 1500]);
trainingErrorReg(1,:) = [1 1];
aiccReg(1,:) = [1 1];
C = log(2*pi)+1;
<span class="keyword">for</span> i = 1:9
<span class="comment">%     i</span>
    X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), i);
    [n, featCount] = size(X);
    b = zeros(featCount,1);
    xPlot = X;
    X(:,1) = [];
    <span class="comment">% Using Functional Minimization Unconstrained function, this time for Logistic Regression to find the beta values</span>
    [beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminuncReg(beta, X, y, lambda)), b, options);
    subplot(3,3,i);
    gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
    hold <span class="string">on</span>;
    f = @(x,y) beta' * ExTwoFunctions.mapFeature(x,y,i)';
    fimplicit(f);
    curMSE = ExTwoFunctions.costLogisticReg(beta,X,y,lambda);
    aiccReg(i,:) = [i, n*log(curMSE) + 2*featCount + n*C];
    trainingErrorReg(i,:) = [i sum(((ExTwoFunctions.sigmoid((xPlot) * beta))&gt;= 0.5) ~= y)];
    title(strcat(<span class="string">'Degree:'</span>,32,int2str(i),32,<span class="string">'Error:'</span>,32,num2str(trainingErrorReg(i,2))));
    legend(<span class="string">'Flawed (0)'</span>, <span class="string">'OK (1)'</span>, <span class="string">'Decision Boundary'</span>);
<span class="keyword">end</span>
snapnow;
close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise9_01.png" alt=""> <h2 id="4">Q3. Making a plot on training error</h2><p>Note that the training error for current model was already calculated in the previous section, and the values were stored in <b>trainingErrorReg</b> matrix. Similarly, the training error for the model in Exercise-6 was created in that exercise, and the values were stored in <b>trainingErrorArray</b> matrix, which was in turn saved in the file <i>tea.mat</i> which is imported here for the purpose of graph plotting.</p><pre class="codeinput">test = exist(<span class="string">'tea.mat'</span>, <span class="string">'file'</span>);
<span class="keyword">if</span> test == 0
    clearvars <span class="string">error</span>;
    error(<span class="string">'The file tea.mat doesn''t exist. Please publish Exercise6.m to generate tea.mat'</span>);
<span class="keyword">else</span>
    load <span class="string">tea.mat</span>;
    hFig = figure(3);
    p = plot(trainingErrorArray(:,1), trainingErrorArray(:,2), trainingErrorReg(:,1), trainingErrorReg(:,2),<span class="string">'--'</span>);
    p(1).LineWidth = 2;
    p(2).LineWidth = 2;
    title(<span class="string">'Training Errors'</span>);
    xlabel(<span class="string">'Degree of the model'</span>);
    ylabel(<span class="string">'Training Error Values'</span>);
    legend(<span class="string">'Without Regularization'</span>, <span class="string">'With Regularization'</span>);
    snapnow;
    close(hFig);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="Exercise9_02.png" alt=""> <p>As can be seen from the graph above, the training error drops sharply from the first degree to second. This is clearly because the first model just consists of a straight line, and it is not possible to separate such type of data with a mere straight line, whether regularized or unregularized. However we don't see much change in the regularized model after that, as the regularized terms in cost and gradient prevent the model from overfitting. Nevertheless, the non-regularized model keeps decreasing constantly after that, with the lowest value when degree is 8, which is tightly overfitted, as can be seen in the graph in exercise 6. A model like this is prone to have a lot of test errors, but we can expect less test errors in the regularized models.</p><h2 id="6">Q4. Cross Validation</h2><p>As it is not mentioned, I have used <b>Akaike's Information Criterion</b> method to calculate the cross validation for each model. The AIC cross validation for the models in Exercise-6 was calculated there itself, and their values were stored in the file <b>tea.mat</b>, which is imported here to plot the graphs.</p><pre class="codeinput"><span class="keyword">if</span> test == 0
    clearvars <span class="string">error</span>;
    error(<span class="string">'The file tea.mat doesn''t exist. Please publish Exercise6.m to generate tea.mat'</span>);
<span class="keyword">else</span>
    hFig = figure(4);
    q = plot(aiccNoReg(:,1), aiccNoReg(:,2), aiccReg(:,1), aiccReg(:,2),<span class="string">'--'</span>);
    q(1).LineWidth = 2;
    q(2).LineWidth = 2;
    title(<span class="string">'Akaike''s Information Criterion Values'</span>);
    xlabel(<span class="string">'Degree of the model'</span>);
    ylabel(<span class="string">'AIC'</span>);
    legend(<span class="string">'Without Normalization'</span>, <span class="string">'With Normalization'</span>);
    snapnow;
    close(hFig);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="Exercise9_03.png" alt=""> <p>As we can see, the the validation error values of the models wthout regularization (let's say WR) is always lesser than of those models with regularization (say R). This is simply because the value of AIC is proportional to the <b>Cost</b> (or MSE for regression models), i.e. the value will always increase with the increase of Cost (because logarithmic function always increases with x when x is greater than 0). However, since we know that Cost values will always be lesser for WR-Models (as they are overfitted), it may not be a good idea to compare WR models with R models. Although we can compare both the models separately. Following were my deductions based only on the observation of the graph.</p><div><ul><li><b>Non-Regularized Model:</b> Eventhough AIC awards penalty if the number of features increase, we can still see constant drops in the AIC values in this case (because the cost is decerasing constantly). This tells us how much the model has overfit the data. However, if we are to choose the best model amongst these, the model with Degree-6 is the winner with the least validation error as compared to other models.</li></ul></div><div><ul><li><b>Regularized Model:</b> As the validation error constatly and subtly increases with the number of features, we can say that the value of <b>Cost</b> never overpowers the equation. In other words, this model never overfits the data. Thus we can make a logical inference that this model is better than the previous model. Moreover according to the validation error values, the best model is the one with Degree-2, as it has the least validation error amongst the others. This is only fair and logical, because if we see the decision boundaries for these models above, we observe that there is no significant difference in the decision boundaries in all the models from Degree-2 to Degree-9. And if this is the case, then choosing a model with more features (which will consume more computational time, while producing more or less the same result) makes no sense.</li></ul></div><h2 id="8">Q5. Trying a larger value of <img src="Exercise9_eq07657233533591063549.png" alt="$\lambda$"></h2><pre class="codeinput">newLambda = 15;
hFig = figure(5);
set(hFig, <span class="string">'Position'</span>, [0 0 1500 1500]);
trainingErrorReg(1,:) = [1 1];
<span class="keyword">for</span> i = 1:9
<span class="comment">%     i</span>
    X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), i);
    [n, featCount] = size(X);
    b = zeros(featCount,1);
    xPlot = X;
    X(:,1) = [];
    [beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminuncReg(beta, X, y, newLambda)), b, options);
    subplot(3,3,i);
    gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
    hold <span class="string">on</span>;
    f = @(x,y) beta' * ExTwoFunctions.mapFeature(x,y,i)';
    fimplicit(f);
    trainingErrorReg(i,:) = [i sum(((ExTwoFunctions.sigmoid((xPlot) * beta))&gt;= 0.5) ~= y)];
    title(strcat(<span class="string">'Degree:'</span>,32,int2str(i),32,<span class="string">'Error:'</span>,32,num2str(trainingErrorReg(i,2))));
    legend(<span class="string">'Flawed (0)'</span>, <span class="string">'OK (1)'</span>, <span class="string">'Decision Boundary'</span>);
<span class="keyword">end</span>
snapnow;
close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise9_04.png" alt=""> <p>A larger value of <img src="Exercise9_eq07657233533591063549.png" alt="$\lambda$"> essentially makes all the shapes almost similar. This can be considered a case of <b>high bias</b>. Boundaries in graphs with Degree 2 &amp; 3 might have some distinct identities, however other boundaries (Degrees 4 to 9) appear identical. I think higher values of <img src="Exercise9_eq07657233533591063549.png" alt="$\lambda$"> may not give us a practical solution, because although it prevents overfitting, it completely destroys unique attributes that are provided by features of higher degree. Another thing we can observe is that the training errors in all the above models are way too much as compared to those modes with a lower value of <img src="Exercise9_eq07657233533591063549.png" alt="$\lambda$">. This re-affirms our contention that a higher value of <img src="Exercise9_eq07657233533591063549.png" alt="$\lambda$"> does not produce better results.</p><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Exercise 9: Regularization
% Submitted by *Prasannjeet Singh*
%
% <html>
%   <link rel="stylesheet" type="text/css" href="../Data/layout.css">
% </html>
%
%% Q1. Implementing costLogisticReg() and gradientDescentReg()
% The functions are implemented and are available in this folder.
%
% Using the cost function (regularization) to calculate cost. The $\lambda$
% value was considered to be 1.

load Data/microchiptests.csv;
X = [ones(size(microchiptests,1),1) microchiptests(:,1:end-1)];
y = microchiptests(:,end);
b = zeros(size(X,2),1);
lambda = 1;
X(:,1) = [];
options = optimset('GradObj', 'on', 'MaxIter', 1000,'Display','off');
theCost = ExTwoFunctions.costLogisticReg(b,X,y,1)

%%
% Using the above implemented gradient descent to calculate the beta and
% cost

a = 0.1;
[betaGradientDescent, costArray, ~] = ExTwoFunctions.gradientDescentReg(X,y,a,lambda);
betaGradientDescent
costGradientDescent = costArray(end,2)

%% Q2. Gradient Descent with the Regularization Term
% As per the question, the gradient descent has been performed in the same way as done in Exercise-6

hFig = figure(2);
set(hFig, 'Position', [0 0 1500 1500]);
trainingErrorReg(1,:) = [1 1];
aiccReg(1,:) = [1 1];
C = log(2*pi)+1;
for i = 1:9
%     i
    X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), i);
    [n, featCount] = size(X);
    b = zeros(featCount,1);
    xPlot = X;
    X(:,1) = [];
    % Using Functional Minimization Unconstrained function, this time for Logistic Regression to find the beta values
    [beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminuncReg(beta, X, y, lambda)), b, options);
    subplot(3,3,i);
    gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
    hold on;
    f = @(x,y) beta' * ExTwoFunctions.mapFeature(x,y,i)';
    fimplicit(f);
    curMSE = ExTwoFunctions.costLogisticReg(beta,X,y,lambda);
    aiccReg(i,:) = [i, n*log(curMSE) + 2*featCount + n*C];
    trainingErrorReg(i,:) = [i sum(((ExTwoFunctions.sigmoid((xPlot) * beta))>= 0.5) ~= y)];
    title(strcat('Degree:',32,int2str(i),32,'Error:',32,num2str(trainingErrorReg(i,2)))); 
    legend('Flawed (0)', 'OK (1)', 'Decision Boundary');
end
snapnow;
close(hFig);

%% Q3. Making a plot on training error
% Note that the training error for current model was already calculated in
% the previous section, and the values were stored in *trainingErrorReg*
% matrix. Similarly, the training error for the model in Exercise-6 was
% created in that exercise, and the values were stored in
% *trainingErrorArray* matrix, which was in turn saved in the file
% _tea.mat_ which is imported here for the purpose of graph plotting.

test = exist('tea.mat', 'file');
if test == 0
    clearvars error;
    error('The file tea.mat doesn''t exist. Please publish Exercise6.m to generate tea.mat');
else
    load tea.mat;
    hFig = figure(3);
    p = plot(trainingErrorArray(:,1), trainingErrorArray(:,2), trainingErrorReg(:,1), trainingErrorReg(:,2),'REPLACE_WITH_DASH_DASH');
    p(1).LineWidth = 2;
    p(2).LineWidth = 2;
    title('Training Errors');
    xlabel('Degree of the model');
    ylabel('Training Error Values');
    legend('Without Regularization', 'With Regularization');
    snapnow;
    close(hFig);
end

%%
% As can be seen from the graph above, the training error drops sharply
% from the first degree to second. This is clearly because the first model
% just consists of a straight line, and it is not possible to separate such
% type of data with a mere straight line, whether regularized or
% unregularized. However we don't see much change in the regularized model
% after that, as the regularized terms in cost and gradient prevent the
% model from overfitting. Nevertheless, the non-regularized model keeps
% decreasing constantly after that, with the lowest value when degree is 8,
% which is tightly overfitted, as can be seen in the graph in exercise 6. A
% model like this is prone to have a lot of test errors, but we can expect
% less test errors in the regularized models.

%% Q4. Cross Validation
% As it is not mentioned, I have used *Akaike's Information Criterion*
% method to calculate the cross validation for each model. The AIC cross
% validation for the models in Exercise-6 was calculated there itself, and
% their values were stored in the file *tea.mat*, which is imported here to
% plot the graphs.

if test == 0
    clearvars error;
    error('The file tea.mat doesn''t exist. Please publish Exercise6.m to generate tea.mat');
else
    hFig = figure(4);
    q = plot(aiccNoReg(:,1), aiccNoReg(:,2), aiccReg(:,1), aiccReg(:,2),'REPLACE_WITH_DASH_DASH');
    q(1).LineWidth = 2;
    q(2).LineWidth = 2;
    title('Akaike''s Information Criterion Values');
    xlabel('Degree of the model');
    ylabel('AIC');
    legend('Without Normalization', 'With Normalization');
    snapnow;
    close(hFig);
end

%%
% As we can see, the the validation error values of the models wthout
% regularization (let's say WR) is always lesser than of those models with
% regularization (say R). This is simply because the value of AIC is
% proportional to the *Cost* (or MSE for regression models), i.e. the value
% will always increase with the increase of Cost (because logarithmic
% function always increases with x when x is greater than 0). However,
% since we know that Cost values will always be lesser for WR-Models (as
% they are overfitted), it may not be a good idea to compare WR models with
% R models. Although we can compare both the models separately. Following
% were my deductions based only on the observation of the graph.
%
% * *Non-Regularized Model:* Eventhough AIC awards penalty if the number of
% features increase, we can still see constant drops in the AIC values in
% this case (because the cost is decerasing constantly). This tells us how
% much the model has overfit the data. However, if we are to choose the
% best model amongst these, the model with Degree-6 is the winner with the
% least validation error as compared to other models.
%
% * *Regularized Model:* As the validation error constatly and subtly
% increases with the number of features, we can say that the value of
% *Cost* never overpowers the equation. In other words, this model never
% overfits the data. Thus we can make a logical inference that this model
% is better than the previous model. Moreover according to the validation
% error values, the best model is the one with Degree-2, as it has the
% least validation error amongst the others. This is only fair and logical,
% because if we see the decision boundaries for these models above, we
% observe that there is no significant difference in the decision
% boundaries in all the models from Degree-2 to Degree-9. And if this is
% the case, then choosing a model with more features (which will consume
% more computational time, while producing more or less the same result)
% makes no sense.

%% Q5. Trying a larger value of $\lambda$

newLambda = 15;
hFig = figure(5);
set(hFig, 'Position', [0 0 1500 1500]);
trainingErrorReg(1,:) = [1 1];
for i = 1:9
%     i
    X = ExTwoFunctions.mapFeature(microchiptests(:,1), microchiptests(:,2), i);
    [n, featCount] = size(X);
    b = zeros(featCount,1);
    xPlot = X;
    X(:,1) = [];
    [beta, final_cost, exitFlag, output] = fminunc(@(beta) (ExTwoFunctions.costFunctionFminuncReg(beta, X, y, newLambda)), b, options);
    subplot(3,3,i);
    gscatter(microchiptests(:,1), microchiptests(:,2), microchiptests(:,3));
    hold on;
    f = @(x,y) beta' * ExTwoFunctions.mapFeature(x,y,i)';
    fimplicit(f);
    trainingErrorReg(i,:) = [i sum(((ExTwoFunctions.sigmoid((xPlot) * beta))>= 0.5) ~= y)];
    title(strcat('Degree:',32,int2str(i),32,'Error:',32,num2str(trainingErrorReg(i,2)))); 
    legend('Flawed (0)', 'OK (1)', 'Decision Boundary');
end
snapnow;
close(hFig);



%%
% A larger value of $\lambda$ essentially makes all the shapes almost
% similar. This can be considered a case of *high bias*. Boundaries in
% graphs with Degree 2 & 3 might have some distinct identities, however
% other boundaries (Degrees 4 to 9) appear identical. I think higher values
% of $\lambda$ may not give us a practical solution, because although it
% prevents overfitting, it completely destroys unique attributes that are
% provided by features of higher degree. Another thing we can observe is
% that the training errors in all the above models are way too much as
% compared to those modes with a lower value of $\lambda$. This re-affirms
% our contention that a higher value of $\lambda$ does not produce better
% results.
##### SOURCE END #####
--></body></html>