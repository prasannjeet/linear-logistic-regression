
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Exercise 4: The Clever Football Coach (Logistic Regression)</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-05-06"><meta name="DC.source" content="Exercise4.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Exercise 4: The Clever Football Coach (Logistic Regression)</h1><!--introduction--><p>Submitted by <b>Prasannjeet Singh</b></p><p>
  <link rel="stylesheet" type="text/css" href="../Data/layout.css">
</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Q1. The Sigmoid Function</a></li><li><a href="#4">Q2. Implementing the cost function</a></li><li><a href="#5">Q3. Normalizing the data and implementing the gradient descent</a></li><li><a href="#7">Q4. Training the data using Gradient Descent.</a></li><li><a href="#9">Q5. The decision boundary</a></li><li><a href="#10">Q6. Training Error</a></li></ul></div><h2 id="1">Q1. The Sigmoid Function</h2><p>The function <b>sigmoid()</b> is implemented and can be found in this folder. Calling the function with the given data:</p><pre class="codeinput">ExTwoFunctions.sigmoid([0 1; 2 3])
</pre><pre class="codeoutput">
ans =

    0.5000    0.7311
    0.8808    0.9526

</pre><p>As can be seen, the solution is concurrent with the one provided in the assignment.</p><p>For the term <b>sigmoid(z)-1</b> to be equal to zero, the value of <b>sigmoid(z)</b> should be equal to 1. We know that , the sigmoid function is asymptotic at y = 1, i.e. it always goes closer to 1, but never touches it. Let us find the number for which matlab interprets the sigmoid function to be 1. For that, we will create an infinite loop and break it when the value of sigmoid function returns 1.</p><pre class="codeinput">z = 0;
loopCondition = true;
<span class="keyword">while</span> loopCondition
    z = z+1;
    <span class="keyword">if</span> ExTwoFunctions.sigmoid(z) == 1
        loopCondition = false;
    <span class="keyword">end</span>
<span class="keyword">end</span>

z
</pre><pre class="codeoutput">
z =

    37

</pre><p>Therefore, after z = <b>37</b>, MATLAB interprets <b>sigmoid(z)-1</b> as zero.</p><h2 id="4">Q2. Implementing the cost function</h2><p>The cost function <b>costLogistic()</b> has been implemented and is present in this folder. Now applying the cost function to the given data:</p><pre class="codeinput">load <span class="string">Data/football.mat</span>;
[n,featureCount] = size(football_X);
X = [ones(n,1) football_X];
b = ones(featureCount+1,1)*0;
<span class="comment">% b=[2.8342 -5.2340 -1.8305]';</span>
cost = ExTwoFunctions.costLogistic(X, football_y, b)
</pre><pre class="codeoutput">
cost =

    0.6931

</pre><h2 id="5">Q3. Normalizing the data and implementing the gradient descent</h2><p><b>Normalizing:</b></p><pre class="codeinput">xNorm = ExTwoFunctions.normalizeData(football_X);
xNorm = [ones(n,1) xNorm];
</pre><p><b>Gradient Descent:</b></p><p>The function <b>logisticGradient()</b> has been implemented and can be found in this folder.</p><h2 id="7">Q4. Training the data using Gradient Descent.</h2><pre class="codeinput">a = 0.000005;
[bGradient, costArray, N] = ExTwoFunctions.logisticGradient(xNorm,football_y,a);
N
</pre><pre class="codeoutput">
N =

      204728

</pre><p>Summarizing:</p><div><ol><li><img src="Exercise4_eq02452474913348637904.png" alt="$\alpha = 0.000005$"></li><li><img src="Exercise4_eq02454436682875872854.png" alt="$N = 204728$"></li></ol></div><p>Plotting the cost function below:</p><pre class="codeinput">hFig = figure(4);
plot(costArray(30:end,1), costArray(30:end,2));
title(<span class="string">'Plotting the Cost vs Iterations'</span>);
xlabel(<span class="string">'Number of Iterations'</span>);
ylabel(<span class="string">'Cost'</span>);
snapnow;
close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise4_01.png" alt=""> <h2 id="9">Q5. The decision boundary</h2><pre class="codeinput">hFig = figure(5);
mu = mean(football_X);
sigma = std(football_X);
plotFootball(football_X, football_y, bGradient, mu, sigma);
snapnow;
close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise4_02.png" alt=""> <h2 id="10">Q6. Training Error</h2><p>Finding both training error and accuracy below</p><pre class="codeinput">trainingError = sum(((ExTwoFunctions.sigmoid(([ones(n,1) ExTwoFunctions.normalizeData(football_X)]) * bGradient))&gt;= 0.5) ~= football_y)
accuracy = (n - trainingError)*100/n
</pre><pre class="codeoutput">
trainingError =

    14


accuracy =

   96.2264

</pre><p>Summarizing:</p><div><ol><li>Training Error = 14</li><li>Accuracy = 96.2264</li></ol></div><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Exercise 4: The Clever Football Coach (Logistic Regression)
% Submitted by *Prasannjeet Singh*
%
% <html>
%   <link rel="stylesheet" type="text/css" href="../Data/layout.css">
% </html>
%
%% Q1. The Sigmoid Function
% The function *sigmoid()* is implemented and can be found in this folder.
% Calling the function with the given data:

ExTwoFunctions.sigmoid([0 1; 2 3])

%%
% As can be seen, the solution is concurrent with the one provided in the
% assignment.
%
% For the term *sigmoid(z)-1* to be equal to zero, the value of
% *sigmoid(z)* should be equal to 1. We know that , the sigmoid function is
% asymptotic at y = 1, i.e. it always goes closer to 1, but never touches
% it. Let us find the number for which matlab interprets the sigmoid
% function to be 1. For that, we will create an infinite loop and break it
% when the value of sigmoid function returns 1.

z = 0;
loopCondition = true;
while loopCondition
    z = z+1;
    if ExTwoFunctions.sigmoid(z) == 1
        loopCondition = false;
    end
end

z

%%
% Therefore, after z = *37*, MATLAB interprets *sigmoid(z)-1* as zero.

%% Q2. Implementing the cost function
% The cost function *costLogistic()* has been implemented and is present in
% this folder. Now applying the cost function to the given data:

load Data/football.mat;
[n,featureCount] = size(football_X);
X = [ones(n,1) football_X];
b = ones(featureCount+1,1)*0;
% b=[2.8342 -5.2340 -1.8305]';
cost = ExTwoFunctions.costLogistic(X, football_y, b)

%% Q3. Normalizing the data and implementing the gradient descent
%
% *Normalizing:*

xNorm = ExTwoFunctions.normalizeData(football_X);
xNorm = [ones(n,1) xNorm];

%%
% *Gradient Descent:*
%
% The function *logisticGradient()* has been implemented and can be found
% in this folder.

%% Q4. Training the data using Gradient Descent.

a = 0.000005;
[bGradient, costArray, N] = ExTwoFunctions.logisticGradient(xNorm,football_y,a);
N

%%
% Summarizing:
%
% # $\alpha = 0.000005$
% # $N = 204728$
%
% Plotting the cost function below:

hFig = figure(4);
plot(costArray(30:end,1), costArray(30:end,2));
title('Plotting the Cost vs Iterations');
xlabel('Number of Iterations');
ylabel('Cost');
snapnow;
close(hFig);

%% Q5. The decision boundary

hFig = figure(5);
mu = mean(football_X);
sigma = std(football_X);
plotFootball(football_X, football_y, bGradient, mu, sigma);
snapnow;
close(hFig);

%% Q6. Training Error
% Finding both training error and accuracy below

trainingError = sum(((ExTwoFunctions.sigmoid(([ones(n,1) ExTwoFunctions.normalizeData(football_X)]) * bGradient))>= 0.5) ~= football_y)
accuracy = (n - trainingError)*100/n

%%
% Summarizing:
%
% # Training Error = 14
% # Accuracy = 96.2264
##### SOURCE END #####
--></body></html>