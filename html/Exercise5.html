
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Exercise 5: Multivariate Logistic Regression</title><meta name="generator" content="MATLAB 9.4"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-05-06"><meta name="DC.source" content="Exercise5.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Exercise 5: Multivariate Logistic Regression</h1><!--introduction--><p>Submitted by <b>Prasannjeet Singh</b></p><p>
  <link rel="stylesheet" type="text/css" href="../Data/layout.css">
</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Q1. Reading the data</a></li><li><a href="#2">Q2. Replacement and separation into training set and test set</a></li><li><a href="#4">Q3. Normalizing, Gradient Descent and Plotting the cost function</a></li><li><a href="#6">Q4. Training Error and Accuracy Percentage</a></li><li><a href="#7">Q5. Test Error and Test Accuracy Percentage</a></li><li><a href="#8">Q6. Repeated Runs</a></li></ul></div><h2 id="1">Q1. Reading the data</h2><pre class="codeinput">data = load(<span class="string">'Data/breast-cancer.mat'</span>);
data = data.breast_cancer;
data = data(randperm(size(data,1)),:); <span class="comment">% Shuffle rows</span>
</pre><h2 id="2">Q2. Replacement and separation into training set and test set</h2><pre class="codeinput"><span class="comment">% Replacing 2's and 4's with 0's and 1's respectively:</span>
data(:,10) = data(:,10) == 4;

<span class="comment">% Below value can be changed if test data are to be varied</span>
totalTestData = 100;
testSet = data(1:totalTestData,:);
data = data(totalTestData+1:end,:);
</pre><p><b>Reason for replacing 2 and 4 with 0 and 1:</b></p><p>As we know, the cost function in logistic regression is:</p><p><img src="Exercise5_eq12297283585892774696.png" alt="$$-{\frac{1}{m}}\sum_{i=1}^{m}\left(&#xA;y^{(i)}log\left(h_{\beta}\left(x^{(i)}\right)\right) + (1-y^{(i)})&#xA;log\left(1-h_{\beta}\left(x^{(i)}\right)\right) \right)$$"></p><p>The above term is basically combined of two different terms. If the value of y is 0, the first term <img src="Exercise5_eq10940754904606762803.png" alt="$y^{(i)}log\left(h_{\beta}\left(x^{(i)}\right)\right)$"> cancells and becomes zero. Similarly, if y is 1, the second term <img src="Exercise5_eq05390133624381818850.png" alt="$(1-y^{(i)}) log\left(1-h_{\beta}\left(x^{(i)}\right)\right)$"> cancels and becomes zero. As it is a logistic regression, this helps us find different cost values for situations when y is 0 and y is 1. In other words, our hypothesis and cost functions are optimized to work when the results are either zero and one, and they won't work if the values are any other numbers, eg. 2 and 4 in this case. This is why we have to convert them into 0 and 1. Moreover, the values 2 and 4 are just categorical values and do not signify anything numerically, so changing them will not have any affect in the sample data sets.</p><p><b>Test and training allocation</b></p><p>As we know that for a model to work efficiently, the training data should be significant, and that is why I have chosen 583 amongst 683 samples to be the training data, and 100 samples as the test data.</p><h2 id="4">Q3. Normalizing, Gradient Descent and Plotting the cost function</h2><pre class="codeinput">y = data(:,end);
data = data(:,1:end-1);
data = ExTwoFunctions.normalizeData(data);
[n, totalFeatures] = size(data);
data = [ones(n,1) data];
a = 0.0001; <span class="comment">%Defining the alpha value here</span>
[b,costArray, N] = ExTwoFunctions.logisticGradient (data,y,a);
N
</pre><pre class="codeoutput">
N =

       70095

</pre><p>Summarizing above:</p><div><ol><li><img src="Exercise5_eq15366905999653366821.png" alt="$\alpha = 0.0001$"></li><li><img src="Exercise5_eq09167848679731590993.png" alt="$N = Shown Above$"></li></ol></div><p>Plotting the cost function:</p><pre class="codeinput">hFig = figure(3);
plot(costArray(:,1), costArray(:,2));
title(<span class="string">'Plotting the Cost vs Iterations'</span>);
xlabel(<span class="string">'Number of Iterations'</span>);
ylabel(<span class="string">'Cost'</span>);
snapnow;
close(hFig);
</pre><img vspace="5" hspace="5" src="Exercise5_01.png" alt=""> <h2 id="6">Q4. Training Error and Accuracy Percentage</h2><pre class="codeinput">trainingError = sum(((ExTwoFunctions.sigmoid((data) * b))&gt;= 0.5) ~= y)
trainingAccuracy = (n - trainingError)*100/n
</pre><pre class="codeoutput">
trainingError =

    17


trainingAccuracy =

   97.0840

</pre><h2 id="7">Q5. Test Error and Test Accuracy Percentage</h2><pre class="codeinput">testSolution = testSet(:,end);
testSet(:,end) = [];
testSet = [ones(totalTestData,1) ExTwoFunctions.normalizeData(testSet)];

testError = sum(((ExTwoFunctions.sigmoid((testSet) * b))&gt;= 0.5) ~= testSolution)
testAccuracy = (totalTestData - testError)*100/totalTestData
</pre><pre class="codeoutput">
testError =

     2


testAccuracy =

    98

</pre><h2 id="8">Q6. Repeated Runs</h2><p>Repeating the runs has an effect on all the parameters which are:</p><div><ol><li>Number of iterations <b>N</b></li><li>Training Error</li><li>Training Accuracy</li><li>Test Error</li><li>Test Accuracy</li></ol></div><p>Re-running the file without changing anything does not have a significant change and more often than not, the error/accuracy are the same. However, if the number of test data are manipulated, significant changes in the number of iterations as well as error/accuracy can be observed.</p><p>Yes, the difference in training and testing is very much expected, as both the data are separate and mutually exclusive. Moreover any hypothesis is prone to a certain extent of errors, as it is impossible to categorize each data sets accurately, therefore, we can observe some differences.</p><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Exercise 5: Multivariate Logistic Regression
% Submitted by *Prasannjeet Singh*
%
% <html>
%   <link rel="stylesheet" type="text/css" href="../Data/layout.css">
% </html>
%
%% Q1. Reading the data

data = load('Data/breast-cancer.mat');
data = data.breast_cancer;
data = data(randperm(size(data,1)),:); % Shuffle rows

%% Q2. Replacement and separation into training set and test set

% Replacing 2's and 4's with 0's and 1's respectively:
data(:,10) = data(:,10) == 4;

% Below value can be changed if test data are to be varied
totalTestData = 100;
testSet = data(1:totalTestData,:);
data = data(totalTestData+1:end,:);

%%
% *Reason for replacing 2 and 4 with 0 and 1:*
%
% As we know, the cost function in logistic regression is:
%
% $$-{\frac{1}{m}}\sum_{i=1}^{m}\left(
% y^{(i)}log\left(h_{\beta}\left(x^{(i)}\right)\right) + (1-y^{(i)})
% log\left(1-h_{\beta}\left(x^{(i)}\right)\right) \right)$$
% 
% The above term is basically combined of two different terms. If the value
% of y is 0, the first term
% $y^{(i)}log\left(h_{\beta}\left(x^{(i)}\right)\right)$ cancells and
% becomes zero. Similarly, if y is 1, the second term $(1-y^{(i)})
% log\left(1-h_{\beta}\left(x^{(i)}\right)\right)$ cancels and becomes
% zero. As it is a logistic regression, this helps us find different cost
% values for situations when y is 0 and y is 1. In other words, our
% hypothesis and cost functions are optimized to work when the results are
% either zero and one, and they won't work if the values are any other
% numbers, eg. 2 and 4 in this case. This is why we have to convert them
% into 0 and 1. Moreover, the values 2 and 4 are just categorical values
% and do not signify anything numerically, so changing them will not have
% any affect in the sample data sets.
%
% *Test and training allocation*
%
% As we know that for a model to work efficiently, the training data should
% be significant, and that is why I have chosen 583 amongst 683 samples to
% be the training data, and 100 samples as the test data.

%% Q3. Normalizing, Gradient Descent and Plotting the cost function

y = data(:,end);
data = data(:,1:end-1);
data = ExTwoFunctions.normalizeData(data);
[n, totalFeatures] = size(data);
data = [ones(n,1) data];
a = 0.0001; %Defining the alpha value here
[b,costArray, N] = ExTwoFunctions.logisticGradient (data,y,a);
N

%%
% Summarizing above:
%
% # $\alpha = 0.0001$
% # $N = Shown Above$
%
% Plotting the cost function:

hFig = figure(3);
plot(costArray(:,1), costArray(:,2));
title('Plotting the Cost vs Iterations');
xlabel('Number of Iterations');
ylabel('Cost');
snapnow;
close(hFig);

%% Q4. Training Error and Accuracy Percentage

trainingError = sum(((ExTwoFunctions.sigmoid((data) * b))>= 0.5) ~= y)
trainingAccuracy = (n - trainingError)*100/n

%% Q5. Test Error and Test Accuracy Percentage

testSolution = testSet(:,end);
testSet(:,end) = [];
testSet = [ones(totalTestData,1) ExTwoFunctions.normalizeData(testSet)];

testError = sum(((ExTwoFunctions.sigmoid((testSet) * b))>= 0.5) ~= testSolution)
testAccuracy = (totalTestData - testError)*100/totalTestData

%% Q6. Repeated Runs
% Repeating the runs has an effect on all the parameters which are:
%
% # Number of iterations *N*
% # Training Error
% # Training Accuracy
% # Test Error
% # Test Accuracy
%
% Re-running the file without changing anything does not have a significant
% change and more often than not, the error/accuracy are the same. However,
% if the number of test data are manipulated, significant changes in the
% number of iterations as well as error/accuracy can be observed.
%
% Yes, the difference in training and testing is very much expected, as
% both the data are separate and mutually exclusive. Moreover any
% hypothesis is prone to a certain extent of errors, as it is impossible to
% categorize each data sets accurately, therefore, we can observe some
% differences.
##### SOURCE END #####
--></body></html>